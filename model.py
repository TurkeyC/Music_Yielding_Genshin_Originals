import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import torch.nn.functional as F
import numpy as np
import json
import time
import os
from datetime import datetime, timedelta
import matplotlib.pyplot as plt
import threading

# æ£€æŸ¥CUDAå¯ç”¨æ€§
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ä½¿ç”¨è®¾å¤‡: {device}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name()}")
    print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")

class HoyoMusicLSTM(nn.Module):
    """åŸºäºPyTorchçš„HoyoMusic LSTMæ¨¡å‹"""
    def __init__(self, vocab_size, seq_length, embedding_dim=256, lstm_units=512):
        super(HoyoMusicLSTM, self).__init__()
        self.vocab_size = vocab_size
        self.seq_length = seq_length
        self.embedding_dim = embedding_dim
        self.lstm_units = lstm_units
        
        # åµŒå…¥å±‚
        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)
          # LSTMå±‚ - ä¿®å¤dropoutè­¦å‘Š
        self.lstm1 = nn.LSTM(embedding_dim, lstm_units, batch_first=True, num_layers=2, dropout=0.3)
        self.bn1 = nn.BatchNorm1d(lstm_units)
        
        self.lstm2 = nn.LSTM(lstm_units, lstm_units, batch_first=True, num_layers=2, dropout=0.3)
        self.bn2 = nn.BatchNorm1d(lstm_units)
        
        self.lstm3 = nn.LSTM(lstm_units, lstm_units, batch_first=True, num_layers=2, dropout=0.3)
        self.bn3 = nn.BatchNorm1d(lstm_units)
        
        # å…¨è¿æ¥å±‚
        self.fc1 = nn.Linear(lstm_units, lstm_units)
        self.dropout1 = nn.Dropout(0.5)
        
        self.fc2 = nn.Linear(lstm_units, lstm_units // 2)
        self.dropout2 = nn.Dropout(0.3)
        
        # è¾“å‡ºå±‚
        self.output = nn.Linear(lstm_units // 2, vocab_size)
        
    def forward(self, x):
        # åµŒå…¥
        embedded = self.embedding(x)  # (batch, seq_len, embedding_dim)
        
        # ç¬¬ä¸€å±‚LSTM
        lstm_out, _ = self.lstm1(embedded)
        # æ‰¹é‡å½’ä¸€åŒ–éœ€è¦è°ƒæ•´ç»´åº¦: (batch, seq_len, features) -> (batch, features, seq_len)
        if lstm_out.size(1) > 1:  # é¿å…å•ä¸ªæ—¶é—´æ­¥çš„æƒ…å†µ
            lstm_out = lstm_out.transpose(1, 2)
            lstm_out = self.bn1(lstm_out)
            lstm_out = lstm_out.transpose(1, 2)
        
        # ç¬¬äºŒå±‚LSTM
        lstm_out, _ = self.lstm2(lstm_out)
        if lstm_out.size(1) > 1:
            lstm_out = lstm_out.transpose(1, 2)
            lstm_out = self.bn2(lstm_out)
            lstm_out = lstm_out.transpose(1, 2)
        
        # ç¬¬ä¸‰å±‚LSTM
        lstm_out, (hidden, _) = self.lstm3(lstm_out)
        if lstm_out.size(1) > 1:
            lstm_out = lstm_out.transpose(1, 2)
            lstm_out = self.bn3(lstm_out)
            lstm_out = lstm_out.transpose(1, 2)
        
        # ä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        last_output = lstm_out[:, -1, :]  # (batch, lstm_units)
        
        # å…¨è¿æ¥å±‚
        out = F.relu(self.fc1(last_output))
        out = self.dropout1(out)
        
        out = F.relu(self.fc2(out))
        out = self.dropout2(out)
        
        # è¾“å‡ºå±‚
        out = self.output(out)  # (batch, vocab_size)
        
        return out

class HoyoMusicGenerator:
    def __init__(self, vocab_size, seq_length, embedding_dim=256, lstm_units=512):
        self.vocab_size = vocab_size
        self.seq_length = seq_length
        self.embedding_dim = embedding_dim
        self.lstm_units = lstm_units
        self.model = None
        self.optimizer = None
        self.criterion = None
        self.scheduler = None
        self.training_history = {'loss': [], 'accuracy': [], 'val_loss': [], 'val_accuracy': []}
        self.training_start_time = None
        self.estimated_time_remaining = None
        # æ–°å¢ï¼šcheckpointå’Œæ–­ç‚¹ç»­è¿ç›¸å…³å±æ€§
        self.current_epoch = 0
        self.best_val_loss = float('inf')
        self.patience_counter = 0
        self.checkpoint_interval = 5  # æ¯5ä¸ªepochä¿å­˜ä¸€æ¬¡checkpoint
        self.auto_save_enabled = True
        self.resume_from_checkpoint = False
        self.last_checkpoint_path = None
    
    def build_model(self):
        """æ„å»ºé’ˆå¯¹HoyoMusicä¼˜åŒ–çš„LSTMæ¨¡å‹"""
        self.model = HoyoMusicLSTM(
            self.vocab_size, 
            self.seq_length, 
            self.embedding_dim, 
            self.lstm_units
        ).to(device)
        
        # ä¼˜åŒ–å™¨
        self.optimizer = optim.Adam(
            self.model.parameters(),
            lr=0.001,
            betas=(0.9, 0.999),
            eps=1e-8
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, 
            mode='min', 
            factor=0.5, 
            patience=8, 
            min_lr=0.0001,
            verbose=True
        )
        
        # æŸå¤±å‡½æ•°
        self.criterion = nn.CrossEntropyLoss(ignore_index=0)  # å¿½ç•¥padding
        
        param_count = sum(p.numel() for p in self.model.parameters())        
        print(f"âœ… HoyoMusicæ¨¡å‹å·²æ„å»ºï¼Œå‚æ•°æ•°é‡: {param_count:,}")
        return self.model
    
    def load_model_for_incremental_training(self, model_path, learning_rate=0.0005):
        """åŠ è½½ç°æœ‰æ¨¡å‹è¿›è¡Œå¢é‡è®­ç»ƒ"""
        print(f"ğŸ”„ åŠ è½½ç°æœ‰æ¨¡å‹è¿›è¡Œå¢é‡è®­ç»ƒ: {model_path}")
        
        try:
            # é¦–å…ˆæ„å»ºæ¨¡å‹ç»“æ„
            if self.model is None:
                self.build_model()
            
            # åŠ è½½æ¨¡å‹æƒé‡ - PyTorch 2.6+ å®‰å…¨æ€§æ›´æ–°
            checkpoint = torch.load(model_path, map_location=device, weights_only=False)
            
            if 'model_state_dict' in checkpoint:
                self.model.load_state_dict(checkpoint['model_state_dict'])
                print(f"âœ… æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸ")
                
                # å¦‚æœæœ‰ä¼˜åŒ–å™¨çŠ¶æ€ï¼Œä¹ŸåŠ è½½
                if 'optimizer_state_dict' in checkpoint and self.optimizer:
                    self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
                    print(f"âœ… ä¼˜åŒ–å™¨çŠ¶æ€åŠ è½½æˆåŠŸ")
            else:
                # å…¼å®¹æ—§æ ¼å¼
                self.model.load_state_dict(checkpoint)
                print(f"âœ… æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸï¼ˆå…¼å®¹æ¨¡å¼ï¼‰")
            
            # é‡æ–°è®¾ç½®å­¦ä¹ ç‡è¿›è¡Œå¢é‡è®­ç»ƒ
            for param_group in self.optimizer.param_groups:
                param_group['lr'] = learning_rate
            
            print(f"ğŸ¯ å¢é‡è®­ç»ƒå­¦ä¹ ç‡è®¾ç½®ä¸º: {learning_rate}")
            
            # åŠ è½½å†å²è®­ç»ƒä¿¡æ¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            self.load_training_history()
            
            return True
            
        except Exception as e:
            print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
            print("ğŸ”§ å°†åˆ›å»ºæ–°æ¨¡å‹...")
            self.build_model()
            return False
    
    def save_model(self, model_path, optimizer_path=None, include_optimizer=True):
        """ä¿å­˜æ¨¡å‹"""
        try:
            if include_optimizer and self.optimizer:
                checkpoint = {
                    'model_state_dict': self.model.state_dict(),
                    'optimizer_state_dict': self.optimizer.state_dict(),
                    'vocab_size': self.vocab_size,
                    'seq_length': self.seq_length,
                    'embedding_dim': self.embedding_dim,
                    'lstm_units': self.lstm_units
                }
            else:
                checkpoint = {
                    'model_state_dict': self.model.state_dict(),
                    'vocab_size': self.vocab_size,
                    'seq_length': self.seq_length,
                    'embedding_dim': self.embedding_dim,
                    'lstm_units': self.lstm_units
                }
            
            torch.save(checkpoint, model_path)
            print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜æ¨¡å‹å¤±è´¥: {e}")
    
    def save_training_history(self, history_path='models/training_history.json'):
        """ä¿å­˜è®­ç»ƒå†å²"""
        try:
            with open(history_path, 'w') as f:
                json.dump(self.training_history, f, indent=2)
            print(f"ğŸ“Š è®­ç»ƒå†å²å·²ä¿å­˜åˆ°: {history_path}")
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜è®­ç»ƒå†å²å¤±è´¥: {e}")
    
    def load_training_history(self, history_path='models/training_history.json'):
        """åŠ è½½è®­ç»ƒå†å²"""
        try:
            with open(history_path, 'r') as f:
                self.training_history = json.load(f)
            print(f"ğŸ“ˆ è®­ç»ƒå†å²å·²åŠ è½½ï¼ŒåŒ…å« {len(self.training_history.get('loss', []))} ä¸ªepoch")
        except FileNotFoundError:
            print("ğŸ“ æœªæ‰¾åˆ°è®­ç»ƒå†å²æ–‡ä»¶ï¼Œå°†åˆ›å»ºæ–°çš„å†å²è®°å½•")
            self.training_history = {'loss': [], 'accuracy': [], 'val_loss': [], 'val_accuracy': []}
        except Exception as e:
            print(f"âš ï¸ åŠ è½½è®­ç»ƒå†å²å¤±è´¥: {e}")
            self.training_history = {'loss': [], 'accuracy': [], 'val_loss': [], 'val_accuracy': []}
    
    def estimate_training_time(self, total_samples, batch_size, epochs, validation_split=0.2):
        """é¢„ä¼°è®­ç»ƒæ—¶é—´"""
        # ä¼°ç®—æ¯ä¸ªbatchçš„è®­ç»ƒæ—¶é—´ï¼ˆåŸºäºç»éªŒå€¼ï¼‰
        samples_per_epoch = int(total_samples * (1 - validation_split))
        batches_per_epoch = (samples_per_epoch + batch_size - 1) // batch_size
        
        # åŸºäºæ¨¡å‹å¤æ‚åº¦ä¼°ç®—æ—¶é—´ï¼ˆç§’/batchï¼‰
        base_time_per_batch = 0.15 if device.type == 'cuda' else 0.5  # GPU vs CPU
        
        # æ ¹æ®æ¨¡å‹å‚æ•°è°ƒæ•´
        complexity_factor = (self.lstm_units / 512) * (self.seq_length / 100)
        time_per_batch = base_time_per_batch * complexity_factor
        
        # éªŒè¯æ—¶é—´ï¼ˆé€šå¸¸æ¯”è®­ç»ƒå¿«ï¼‰
        validation_samples = int(total_samples * validation_split)
        validation_batches = (validation_samples + batch_size - 1) // batch_size
        validation_time_per_epoch = validation_batches * time_per_batch * 0.3
        
        # æ€»æ—¶é—´ä¼°ç®—
        training_time_per_epoch = batches_per_epoch * time_per_batch
        total_time_per_epoch = training_time_per_epoch + validation_time_per_epoch
        total_estimated_time = total_time_per_epoch * epochs
        
        return {
            'total_estimated_seconds': total_estimated_time,
            'time_per_epoch': total_time_per_epoch,
            'batches_per_epoch': batches_per_epoch,
            'time_per_batch': time_per_batch
        }
    def format_time(self, seconds):
        """æ ¼å¼åŒ–æ—¶é—´æ˜¾ç¤º"""
        if seconds is None:
            return "æœªçŸ¥æ—¶é—´"
        
        if seconds < 60:
            return f"{seconds:.0f}ç§’"
        elif seconds < 3600:
            minutes = seconds // 60
            remaining_seconds = seconds % 60
            return f"{minutes:.0f}åˆ†{remaining_seconds:.0f}ç§’"
        else:
            hours = seconds // 3600
            remaining_minutes = (seconds % 3600) // 60
            return f"{hours:.0f}å°æ—¶{remaining_minutes:.0f}åˆ†é’Ÿ"
    
    def validate_model(self, val_loader):
        """éªŒè¯æ¨¡å‹"""
        self.model.eval()
        total_loss = 0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for batch_X, batch_y in val_loader:
                batch_X, batch_y = batch_X.to(device), batch_y.to(device)
                
                outputs = self.model(batch_X)
                loss = self.criterion(outputs, batch_y)
                
                total_loss += loss.item()
                
                _, predicted = torch.max(outputs.data, 1)
                total += batch_y.size(0)
                correct += (predicted == batch_y).sum().item()
          # é˜²å¾¡æ€§ç¼–ç¨‹ï¼šå¤„ç†ç©ºæ•°æ®åŠ è½½å™¨æˆ–é™¤é›¶æƒ…å†µ
        if len(val_loader) > 0:
            avg_loss = total_loss / len(val_loader)
        else:
            avg_loss = 0.0
            
        if total > 0:
            accuracy = correct / total
        else:
            accuracy = 0.0
            
        return avg_loss, accuracy    
    def train(self, X, y, epochs=100, batch_size=32, validation_split=0.2, 
              model_save_path='models/hoyomusic_generator.pth', is_incremental=False,
              enable_checkpoints=True, checkpoint_interval=5, auto_resume=False, 
              checkpoint_dir='models/checkpoints'):
        """è®­ç»ƒæ¨¡å‹ï¼ˆæ”¯æŒå¢é‡è®­ç»ƒå’Œæ–­ç‚¹ç»­è¿ï¼‰"""
        
        # å°è¯•è‡ªåŠ¨æ¢å¤è®­ç»ƒï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if auto_resume and not is_incremental:
            if self.auto_resume_training():
                is_incremental = True  # å¦‚æœæ¢å¤æˆåŠŸï¼Œæ ‡è®°ä¸ºå¢é‡è®­ç»ƒ
        
        print(f"ğŸš€ å¼€å§‹{'å¢é‡' if is_incremental else ''}è®­ç»ƒ...")
        
        # å¦‚æœä¸æ˜¯å¢é‡è®­ç»ƒä¸”æ¨¡å‹ä¸å­˜åœ¨ï¼Œåˆ™æ„å»ºæ–°æ¨¡å‹
        if not is_incremental and self.model is None:
            self.build_model()
        
        # è®¾ç½®checkpointå‚æ•°
        self.checkpoint_interval = checkpoint_interval
        self.auto_save_enabled = enable_checkpoints
        
        # è½¬æ¢æ•°æ®åˆ°PyTorchå¼ é‡
        X_tensor = torch.LongTensor(X)
        y_tensor = torch.LongTensor(y)
        
        # åˆ†å‰²æ•°æ®
        n_samples = len(X)
        n_train = int(n_samples * (1 - validation_split))
        
        # åˆ›å»ºæ•°æ®é›†
        train_dataset = TensorDataset(X_tensor[:n_train], y_tensor[:n_train])
        val_dataset = TensorDataset(X_tensor[n_train:], y_tensor[n_train:])
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
        
        print(f"ğŸµ HoyoMusicç”Ÿæˆå™¨æ¨¡å‹æ‘˜è¦:")
        param_count = sum(p.numel() for p in self.model.parameters())
        print(f"  - å‚æ•°æ•°é‡: {param_count:,}")
        print(f"  - è®­ç»ƒæ ·æœ¬: {len(train_dataset):,}")
        print(f"  - éªŒè¯æ ·æœ¬: {len(val_dataset):,}")        # è®¡ç®—å®é™…éœ€è¦è®­ç»ƒçš„epochæ•°ï¼ˆå¦‚æœæ˜¯æ¢å¤è®­ç»ƒï¼‰
        # ç¡®ä¿current_epochéNoneå¹¶ä¸”æ˜¯æ•´æ•°ç±»å‹
        if self.resume_from_checkpoint and isinstance(self.current_epoch, (int, float)):
            start_epoch = self.current_epoch
        else:
            start_epoch = 0
            
        remaining_epochs = epochs - start_epoch
        
        if self.resume_from_checkpoint and start_epoch > 0:
            print(f"ğŸ”„ ä»epoch {start_epoch} æ¢å¤è®­ç»ƒï¼Œè¿˜éœ€è®­ç»ƒ {remaining_epochs} ä¸ªepoch")
        
        # é¢„ä¼°è®­ç»ƒæ—¶é—´
        time_estimates = self.estimate_training_time(len(X), batch_size, remaining_epochs, validation_split)
        
        print(f"\nâ±ï¸  è®­ç»ƒæ—¶é—´é¢„ä¼°:")
        print(f"  - æ¯ä¸ªepoché¢„ä¼°: {self.format_time(time_estimates['time_per_epoch'])}")
        print(f"  - å‰©ä½™è®­ç»ƒæ—¶é—´é¢„ä¼°: {self.format_time(time_estimates['total_estimated_seconds'])}")
        print(f"  - æ¯æ‰¹æ¬¡æ—¶é—´: {time_estimates['time_per_batch']:.2f}ç§’")
        print(f"  - æ¯ä¸ªepochæ‰¹æ¬¡æ•°: {time_estimates['batches_per_epoch']}")
        
        completion_time = datetime.now() + timedelta(seconds=time_estimates['total_estimated_seconds'])
        print(f"  - é¢„è®¡å®Œæˆæ—¶é—´: {completion_time.strftime('%Y-%m-%d %H:%M:%S')}")
        
        # Checkpointè®¾ç½®ä¿¡æ¯
        if enable_checkpoints:
            print(f"\nğŸ’¾ Checkpointè®¾ç½®:")
            print(f"  - è‡ªåŠ¨ä¿å­˜é—´éš”: æ¯ {checkpoint_interval} ä¸ªepoch")
            print(f"  - Checkpointç›®å½•: models/checkpoints/")
          # è®°å½•è®­ç»ƒå¼€å§‹æ—¶é—´ï¼ˆå¦‚æœä¸æ˜¯æ¢å¤è®­ç»ƒï¼‰
        if not self.resume_from_checkpoint:
            self.training_start_time = time.time()
          # åˆå§‹åŒ–å†å²è®°å½•ï¼ˆå¦‚æœä¸æ˜¯å¢é‡è®­ç»ƒï¼‰
        if not is_incremental or not self.training_history:
            self.training_history = {'loss': [], 'accuracy': [], 'val_loss': [], 'val_accuracy': []}
        else:
            # ç¡®ä¿è®­ç»ƒå†å²åŒ…å«æ‰€æœ‰å¿…éœ€çš„é”®
            for key in ['loss', 'accuracy', 'val_loss', 'val_accuracy']:
                if key not in self.training_history:
                    self.training_history[key] = []
        
        # ä½¿ç”¨checkpointä¸­çš„æœ€ä½³æŸå¤±å’Œè€å¿ƒè®¡æ•°å™¨ï¼ˆå¦‚æœæ˜¯æ¢å¤è®­ç»ƒï¼‰
        if not self.resume_from_checkpoint:
            self.best_val_loss = float('inf')
            self.patience_counter = 0
        
        max_patience = 15
        
        # è®­ç»ƒå¾ªç¯ - ä»æ­£ç¡®çš„epochå¼€å§‹
        for epoch in range(start_epoch, epochs):
            self.current_epoch = epoch  # æ›´æ–°å½“å‰epoch
            
            # è®­ç»ƒé˜¶æ®µ
            self.model.train()
            epoch_loss = 0
            correct = 0
            total = 0
            
            for batch_idx, (batch_X, batch_y) in enumerate(train_loader):
                batch_X, batch_y = batch_X.to(device), batch_y.to(device)
                
                # å‰å‘ä¼ æ’­
                self.optimizer.zero_grad()
                outputs = self.model(batch_X)
                loss = self.criterion(outputs, batch_y)
                
                # åå‘ä¼ æ’­
                loss.backward()
                self.optimizer.step()
                
                # ç»Ÿè®¡
                epoch_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                total += batch_y.size(0)
                correct += (predicted == batch_y).sum().item()
            
            # è®¡ç®—è®­ç»ƒæŒ‡æ ‡
            avg_train_loss = epoch_loss / len(train_loader) if len(train_loader) > 0 else 0.0
            train_accuracy = correct / total if total > 0 else 0.0
            
            # éªŒè¯é˜¶æ®µ
            val_loss, val_accuracy = self.validate_model(val_loader)
            # é˜²å¾¡æ€§ä¿®æ­£ï¼šval_lossä¸ºNoneæ—¶èµ‹å€¼ä¸ºfloat('inf')
            if val_loss is None:
                val_loss = float('inf')
            
            # æ›´æ–°å­¦ä¹ ç‡
            self.scheduler.step(val_loss)
            
            # è®°å½•å†å²
            self.training_history['loss'].append(avg_train_loss)
            self.training_history['accuracy'].append(train_accuracy)
            self.training_history['val_loss'].append(val_loss)
            self.training_history['val_accuracy'].append(val_accuracy)
              # æ—¶é—´ä¼°ç®—
            if self.training_start_time:
                elapsed_time = time.time() - self.training_start_time
                
                # æ›´å¤šé˜²å¾¡æ€§æ£€æŸ¥ï¼Œç¡®ä¿æ‰€æœ‰å€¼éƒ½æ˜¯æœ‰æ•ˆæ•°å­—
                if epoch is None:
                    epoch = 0
                if start_epoch is None:
                    start_epoch = 0
                if epochs is None:
                    epochs = 1
                    
                # è®¡ç®—å·²å®Œæˆå’Œå‰©ä½™çš„epochs
                try:
                    epochs_completed = max(0, epoch - start_epoch + 1)
                    remaining_epochs = max(0, epochs - epoch - 1)
                except TypeError:  # æ•è·ä»»ä½•ç±»å‹é”™è¯¯
                    epochs_completed = 0
                    remaining_epochs = 0                
                try:
                    if epochs_completed > 0:
                        avg_time_per_epoch = elapsed_time / epochs_completed
                        self.estimated_time_remaining = avg_time_per_epoch * remaining_epochs
                        completion_time = datetime.now() + timedelta(seconds=self.estimated_time_remaining)
                except (TypeError, ValueError, ZeroDivisionError):
                    # å¦‚æœå‡ºç°ä»»ä½•è®¡ç®—é”™è¯¯ï¼Œå°†ä¼°è®¡æ—¶é—´è®¾ä¸ºNone
                    self.estimated_time_remaining = None
                    completion_time = None
            
            # æ‰“å°è¿›åº¦
            print(f"Epoch {epoch+1}/{epochs}:")
            print(f"  è®­ç»ƒæŸå¤±: {avg_train_loss:.4f} | è®­ç»ƒå‡†ç¡®ç‡: {train_accuracy:.4f}")
            print(f"  éªŒè¯æŸå¤±: {val_loss:.4f} | éªŒè¯å‡†ç¡®ç‡: {val_accuracy:.4f}")
            if hasattr(self, 'estimated_time_remaining'):
                print(f"  å‰©ä½™æ—¶é—´: {self.format_time(self.estimated_time_remaining)}")
            
            # æ—©åœå’Œæ¨¡å‹ä¿å­˜
            is_best_model = False
            # é˜²å¾¡æ€§ä¿®æ­£ï¼šself.best_val_lossä¸ºNoneæˆ–éæ³•ç±»å‹æ—¶èµ‹å€¼ä¸ºfloat('inf')
            if self.best_val_loss is None or not isinstance(self.best_val_loss, (int, float)) or (isinstance(self.best_val_loss, float) and (self.best_val_loss != self.best_val_loss)):
                self.best_val_loss = float('inf')
            if val_loss is None or not isinstance(val_loss, (int, float)) or (isinstance(val_loss, float) and (val_loss != val_loss)):
                val_loss = float('inf')
            if val_loss < self.best_val_loss:
                self.best_val_loss = val_loss
                self.patience_counter = 0
                is_best_model = True
                self.save_model(model_save_path)
                print(f"  âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ (éªŒè¯æŸå¤±: {val_loss:.4f})")
            else:
                self.patience_counter += 1
                
            # Checkpointä¿å­˜
            if enable_checkpoints:
                # å®šæœŸä¿å­˜checkpoint
                if (epoch + 1) % checkpoint_interval == 0:                    checkpoint_info = {
                        'epoch_performance': {
                            'train_loss': avg_train_loss,
                            'train_acc': train_accuracy,
                            'val_loss': val_loss,
                            'val_acc': val_accuracy
                        }
                    }
                self.save_checkpoint(checkpoint_dir=checkpoint_dir, epoch=epoch, extra_info=checkpoint_info)
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹checkpoint
                if is_best_model:                    checkpoint_info = {
                        'best_model': True,
                        'epoch_performance': {
                            'train_loss': avg_train_loss,
                            'train_acc': train_accuracy,
                            'val_loss': val_loss,
                            'val_acc': val_accuracy
                        }
                    }
                self.save_checkpoint(checkpoint_dir=checkpoint_dir, epoch=epoch, is_best=True, extra_info=checkpoint_info)
            # æ—©åœæ£€æŸ¥
            if self.patience_counter >= max_patience:
                print(f"  â¹ï¸ æ—©åœï¼šéªŒè¯æŸå¤±åœ¨ {max_patience} ä¸ªepochå†…æœªæ”¹å–„")
                break
        
        # ä¿å­˜æœ€ç»ˆcheckpoint
        if enable_checkpoints:
            final_checkpoint_info = {
                'training_completed': True,
                'final_epoch': epoch,
                'final_performance': {
                    'train_loss': avg_train_loss,
                    'train_acc': train_accuracy,
                    'val_loss': val_loss,
                    'val_acc': val_accuracy
                }
            }
            self.save_checkpoint(checkpoint_dir=checkpoint_dir, epoch=epoch, extra_info=final_checkpoint_info)
              # æ¸…ç†æ—§çš„checkpoint
            self.cleanup_old_checkpoints(checkpoint_dir=checkpoint_dir)
        
        # ä¿å­˜è®­ç»ƒå†å²
        self.save_training_history()
          # è®¡ç®—å®é™…è®­ç»ƒæ—¶é—´
        actual_training_time = time.time() - self.training_start_time
        print(f"\nâœ… è®­ç»ƒå®Œæˆï¼")
        print(f"ğŸ• å®é™…è®­ç»ƒæ—¶é—´: {self.format_time(actual_training_time)}")
        
        # é˜²å¾¡æ€§æ£€æŸ¥ï¼šç¡®ä¿å†å²è®°å½•ä¸ä¸ºç©ºä¸”åŒ…å«æ‰€æœ‰å¿…éœ€çš„é”®
        for key in ['val_loss', 'val_accuracy', 'loss', 'accuracy']:
            if key not in self.training_history or not self.training_history[key]:
                self.training_history[key] = [0.0]  # æä¾›é»˜è®¤å€¼
        
        print(f"ğŸ“ˆ æœ€ç»ˆéªŒè¯æŸå¤±: {self.training_history['val_loss'][-1]:.4f}")
        print(f"ğŸ¯ æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡: {self.training_history['val_accuracy'][-1]:.4f}")
        
        # åˆ›å»ºå…¼å®¹çš„å†å²å¯¹è±¡
        class HistoryCompat:
            def __init__(self, history_dict):
                self.history = history_dict if history_dict else {'loss': [], 'accuracy': [], 'val_loss': [], 'val_accuracy': []}
        
        return HistoryCompat(self.training_history)
    
    def save_checkpoint(self, checkpoint_dir='models/checkpoints', epoch=None, is_best=False, extra_info=None):
        """ä¿å­˜è®­ç»ƒcheckpoint"""
        import os
        import time
        from datetime import datetime
        
        os.makedirs(checkpoint_dir, exist_ok=True)
        
        if epoch is None:
            epoch = self.current_epoch
            
        # æ„å»ºcheckpointæ–‡ä»¶å
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        if is_best:
            checkpoint_name = f'best_model_epoch_{epoch}_{timestamp}.pth'
        else:
            checkpoint_name = f'checkpoint_epoch_{epoch}_{timestamp}.pth'
            
        checkpoint_path = os.path.join(checkpoint_dir, checkpoint_name)
        
        # å‡†å¤‡checkpointæ•°æ®
        checkpoint_data = {
            # æ¨¡å‹å’Œä¼˜åŒ–å™¨çŠ¶æ€
            'model_state_dict': self.model.state_dict() if self.model else None,
            'optimizer_state_dict': self.optimizer.state_dict() if self.optimizer else None,
            'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,
            
            # æ¨¡å‹é…ç½®
            'vocab_size': self.vocab_size,
            'seq_length': self.seq_length,
            'embedding_dim': self.embedding_dim,
            'lstm_units': self.lstm_units,
            
            # è®­ç»ƒçŠ¶æ€
            'current_epoch': epoch,
            'training_history': self.training_history,
            'best_val_loss': self.best_val_loss,
            'patience_counter': self.patience_counter,
            
            # æ—¶é—´ä¿¡æ¯
            'training_start_time': self.training_start_time,
            'checkpoint_time': time.time(),
            'timestamp': timestamp,
            
            # é¢å¤–ä¿¡æ¯
            'extra_info': extra_info or {},
            
            # ç‰ˆæœ¬ä¿¡æ¯
            'pytorch_version': torch.__version__,
            'checkpoint_version': '2.0'
        }
        
        try:
            torch.save(checkpoint_data, checkpoint_path)
            self.last_checkpoint_path = checkpoint_path
            
            if is_best:
                print(f"ğŸ’ ä¿å­˜æœ€ä½³æ¨¡å‹checkpoint: {checkpoint_name}")
            else:
                print(f"ğŸ’¾ ä¿å­˜è®­ç»ƒcheckpoint: {checkpoint_name}")
                
            # åˆ›å»ºç¬¦å·é“¾æ¥åˆ°æœ€æ–°checkpointï¼ˆä¾¿äºæ¢å¤ï¼‰
            latest_link = os.path.join(checkpoint_dir, 'latest_checkpoint.pth')
            if os.path.exists(latest_link):
                os.remove(latest_link)
            
            # åœ¨Windowsä¸Šåˆ›å»ºå‰¯æœ¬è€Œä¸æ˜¯ç¬¦å·é“¾æ¥
            if os.name == 'nt':  # Windows
                import shutil
                shutil.copy2(checkpoint_path, latest_link)
            else:
                os.symlink(os.path.basename(checkpoint_path), latest_link)
                
            return checkpoint_path
            
        except Exception as e:
            print(f"âŒ ä¿å­˜checkpointå¤±è´¥: {e}")
            return None
    def load_checkpoint(self, checkpoint_path, resume_training=True):
        """åŠ è½½checkpointå¹¶æ¢å¤è®­ç»ƒçŠ¶æ€"""
        try:
            print(f"ğŸ“‚ åŠ è½½checkpoint: {checkpoint_path}")
            
            # PyTorch 2.6+ å®‰å…¨æ€§æ›´æ–°ï¼Œéœ€è¦è®¾ç½®weights_only=False
            checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
            
            # éªŒè¯checkpointç‰ˆæœ¬
            checkpoint_version = checkpoint.get('checkpoint_version', '1.0')
            print(f"ğŸ“‹ Checkpointç‰ˆæœ¬: {checkpoint_version}")
            
            # æ¢å¤æ¨¡å‹é…ç½®
            self.vocab_size = checkpoint.get('vocab_size', self.vocab_size)
            self.seq_length = checkpoint.get('seq_length', self.seq_length)
            self.embedding_dim = checkpoint.get('embedding_dim', self.embedding_dim)
            self.lstm_units = checkpoint.get('lstm_units', self.lstm_units)
            
            # æ„å»ºæ¨¡å‹ï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
            if self.model is None:
                self.build_model()
            
            # åŠ è½½æ¨¡å‹çŠ¶æ€
            if 'model_state_dict' in checkpoint and checkpoint['model_state_dict']:
                self.model.load_state_dict(checkpoint['model_state_dict'])
                print("âœ… æ¨¡å‹æƒé‡å·²æ¢å¤")
            
            if resume_training:                # æ¢å¤è®­ç»ƒçŠ¶æ€
                self.current_epoch = checkpoint.get('current_epoch', 0)
                
                # è·å–å¹¶éªŒè¯è®­ç»ƒå†å²
                checkpoint_history = checkpoint.get('training_history', {})
                default_history = {'loss': [], 'accuracy': [], 'val_loss': [], 'val_accuracy': []}
                
                # å¦‚æœcheckpointæ²¡æœ‰å†å²è®°å½•ï¼Œä½¿ç”¨é»˜è®¤ç©ºå†å²
                if not checkpoint_history:
                    self.training_history = default_history
                else:
                    # ç¡®ä¿æ‰€æœ‰å¿…éœ€çš„é”®éƒ½å­˜åœ¨
                    self.training_history = checkpoint_history
                    for key in default_history:
                        if key not in self.training_history:
                            self.training_history[key] = []
                        # ç¡®ä¿å€¼æ˜¯æœ‰æ•ˆçš„åˆ—è¡¨
                        if not isinstance(self.training_history[key], list):
                            self.training_history[key] = []
                
                # ä¿®å¤ best_val_loss å¯èƒ½ä¸º None æˆ–éæ³•ç±»å‹çš„é—®é¢˜
                best_val_loss = checkpoint.get('best_val_loss', float('inf'))
                if best_val_loss is None or not isinstance(best_val_loss, (int, float)) or (isinstance(best_val_loss, float) and (best_val_loss != best_val_loss)):
                    self.best_val_loss = float('inf')
                else:
                    self.best_val_loss = float(best_val_loss)
                self.patience_counter = checkpoint.get('patience_counter', 0)
                self.training_start_time = checkpoint.get('training_start_time', None)
                
                # æ¢å¤ä¼˜åŒ–å™¨çŠ¶æ€
                if 'optimizer_state_dict' in checkpoint and checkpoint['optimizer_state_dict'] and self.optimizer:
                    self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
                    print("âœ… ä¼˜åŒ–å™¨çŠ¶æ€å·²æ¢å¤")
                
                # æ¢å¤å­¦ä¹ ç‡è°ƒåº¦å™¨çŠ¶æ€
                if 'scheduler_state_dict' in checkpoint and checkpoint['scheduler_state_dict'] and self.scheduler:
                    self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
                    print("âœ… å­¦ä¹ ç‡è°ƒåº¦å™¨çŠ¶æ€å·²æ¢å¤")
                
                self.resume_from_checkpoint = True
                
                print(f"ğŸ”„ è®­ç»ƒçŠ¶æ€å·²æ¢å¤:")
                print(f"  - å½“å‰epoch: {self.current_epoch}")
                print(f"  - æœ€ä½³éªŒè¯æŸå¤±: {self.best_val_loss:.4f}")
                print(f"  - è®­ç»ƒå†å²é•¿åº¦: {len(self.training_history.get('loss', []))}")
                
            else:
                print("ğŸ“– ä»…åŠ è½½æ¨¡å‹æƒé‡ï¼ˆä¸æ¢å¤è®­ç»ƒçŠ¶æ€ï¼‰")
            
            # æ˜¾ç¤ºé¢å¤–ä¿¡æ¯
            extra_info = checkpoint.get('extra_info', {})
            if extra_info:
                print(f"ğŸ“ é¢å¤–ä¿¡æ¯: {extra_info}")
                
            return True
            
        except Exception as e:
            print(f"âŒ åŠ è½½checkpointå¤±è´¥: {e}")
            return False
    def find_latest_checkpoint(self, checkpoint_dir='models/checkpoints'):
        """æŸ¥æ‰¾æœ€æ–°çš„checkpointæ–‡ä»¶"""
        import os
        import glob
        
        if not os.path.exists(checkpoint_dir):
            return None
            
        # é¦–å…ˆå°è¯•latest_checkpoint.pth
        latest_link = os.path.join(checkpoint_dir, 'latest_checkpoint.pth')
        if os.path.exists(latest_link):
            return latest_link
            
        # å¦‚æœæ²¡æœ‰ï¼ŒæŸ¥æ‰¾æœ€æ–°çš„checkpointæ–‡ä»¶
        checkpoint_pattern = os.path.join(checkpoint_dir, 'checkpoint_epoch_*.pth')
        checkpoint_files = glob.glob(checkpoint_pattern)
        
        if not checkpoint_files:
            return None
            
        # è¿‡æ»¤æ‰æ— æ•ˆæ–‡ä»¶å¹¶æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œè¿”å›æœ€æ–°çš„
        valid_files = []
        for file_path in checkpoint_files:
            try:
                if os.path.exists(file_path):
                    mtime = os.path.getmtime(file_path)
                    if mtime is not None:
                        valid_files.append(file_path)
            except (OSError, TypeError):
                continue
                
        if not valid_files:
            return None
            
        latest_checkpoint = max(valid_files, key=os.path.getmtime)
        return latest_checkpoint
    def cleanup_old_checkpoints(self, checkpoint_dir='models/checkpoints', keep_count=5):
        """æ¸…ç†æ—§çš„checkpointæ–‡ä»¶ï¼Œåªä¿ç•™æœ€æ–°çš„å‡ ä¸ª"""
        import os
        import glob
        
        checkpoint_pattern = os.path.join(checkpoint_dir, 'checkpoint_epoch_*.pth')
        checkpoint_files = glob.glob(checkpoint_pattern)
        
        # è¿‡æ»¤æ‰æ— æ•ˆæ–‡ä»¶
        valid_files = []
        for file_path in checkpoint_files:
            try:
                if os.path.exists(file_path):
                    mtime = os.path.getmtime(file_path)
                    if mtime is not None:
                        valid_files.append(file_path)
            except (OSError, TypeError):
                continue
        
        if len(valid_files) <= keep_count:
            return
            
        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åº
        valid_files.sort(key=os.path.getmtime, reverse=True)
        
        # åˆ é™¤æ—§çš„checkpoint
        files_to_delete = valid_files[keep_count:]
        for file_path in files_to_delete:
            try:
                os.remove(file_path)
                print(f"ğŸ—‘ï¸ æ¸…ç†æ—§checkpoint: {os.path.basename(file_path)}")
            except Exception as e:
                print(f"âš ï¸ æ¸…ç†checkpointå¤±è´¥: {e}")
    
    def auto_resume_training(self, checkpoint_dir='models/checkpoints'):
        """è‡ªåŠ¨ä»æœ€æ–°checkpointæ¢å¤è®­ç»ƒ"""
        latest_checkpoint = self.find_latest_checkpoint(checkpoint_dir)
        
        if latest_checkpoint:
            print(f"ğŸ” å‘ç°æœ€æ–°checkpoint: {os.path.basename(latest_checkpoint)}")
            response = input("æ˜¯å¦ä»æ­¤checkpointæ¢å¤è®­ç»ƒï¼Ÿ(y/n): ").lower().strip()
            
            if response in ['y', 'yes', 'æ˜¯']:
                return self.load_checkpoint(latest_checkpoint, resume_training=True)
        
        print("ğŸ“„ æœªæ‰¾åˆ°å¯æ¢å¤çš„checkpointï¼Œå°†å¼€å§‹æ–°çš„è®­ç»ƒ")
        return False
    
    def generate_music(self, seed_text, char_to_int, int_to_char, length=800, temperature=0.8):
        """ç”ŸæˆHoyoMusicé£æ ¼çš„éŸ³ä¹"""
        if self.model is None:
            raise ValueError("æ¨¡å‹æœªåŠ è½½æˆ–æœªè®­ç»ƒ")
        
        self.model.eval()
        
        # å‡†å¤‡ç§å­åºåˆ—
        seed_sequence = [char_to_int.get(char, 0) for char in seed_text[-self.seq_length:]]
        
        # å¦‚æœç§å­åºåˆ—å¤ªçŸ­ï¼Œç”¨ç©ºæ ¼å¡«å……
        while len(seed_sequence) < self.seq_length:
            seed_sequence.insert(0, char_to_int.get(' ', 0))
        
        generated_text = seed_text
        
        # ç”ŸæˆéŸ³ä¹
        with torch.no_grad():
            for i in range(length):
                # é¢„æµ‹ä¸‹ä¸€ä¸ªå­—ç¬¦
                x = torch.LongTensor([seed_sequence]).to(device)
                outputs = self.model(x)
                predictions = F.softmax(outputs, dim=1).cpu().numpy()[0]
                
                # åº”ç”¨æ¸©åº¦å‚æ•°è¿›è¡Œé‡‡æ ·
                predictions = np.log(predictions + 1e-8) / temperature
                exp_preds = np.exp(predictions)
                predictions = exp_preds / np.sum(exp_preds)
                
                # é‡‡æ ·ä¸‹ä¸€ä¸ªå­—ç¬¦
                next_char_idx = np.random.choice(len(predictions), p=predictions)
                next_char = int_to_char[next_char_idx]
                
                generated_text += next_char
                
                # æ›´æ–°ç§å­åºåˆ—
                seed_sequence = seed_sequence[1:] + [next_char_idx]
                
                # å¦‚æœç”Ÿæˆäº†å®Œæ•´çš„æ›²å­æ ‡è®°ï¼Œå¯ä»¥é€‰æ‹©åœæ­¢
                if i > 100 and generated_text.count('X:') > 3:  # ç”Ÿæˆäº†å¤šé¦–æ›²å­
                    break
        
        return generated_text
    
    def generate_hoyomusic_style(self, region="Mondstadt", length=600, temperature=0.8, char_to_int=None, int_to_char=None):
        """ç”Ÿæˆç‰¹å®šåœ°åŒºé£æ ¼çš„åŸç¥éŸ³ä¹"""
        
        # ä¸åŒåœ°åŒºçš„ç§å­æ¨¡æ¿
        region_seeds = {
            "Mondstadt": "X:1\nT:Mondstadt Breeze\nC:Generated by AI\nM:4/4\nL:1/8\nK:C major\n",
            "Liyue": "X:1\nT:Liyue Harbor\nC:Generated by AI\nM:4/4\nL:1/8\nK:A minor\n",
            "Inazuma": "X:1\nT:Inazuma Thunder\nC:Generated by AI\nM:4/4\nL:1/8\nK:D major\n",
            "Sumeru": "X:1\nT:Sumeru Forest\nC:Generated by AI\nM:6/8\nL:1/8\nK:G major\n",
            "Fontaine": "X:1\nT:Fontaine Waters\nC:Generated by AI\nM:3/4\nL:1/4\nK:F major\n"
        }
        
        seed = region_seeds.get(region, region_seeds["Mondstadt"])
        
        return self.generate_music(seed, char_to_int, int_to_char, length, temperature)
    
    def get_model_size(self):
        """è·å–æ¨¡å‹å‚æ•°æ•°é‡"""
        if self.model is None:
            return 0
        
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        
        return trainable_params
    
    def train_step(self, X, y):
        """æ‰§è¡Œå•æ­¥è®­ç»ƒå¹¶è¿”å›æŸå¤±å€¼"""
        if self.model is None:
            raise ValueError("æ¨¡å‹æœªæ„å»ºï¼Œè¯·å…ˆè°ƒç”¨build_model()")
        
        self.model.train()
        
        # è½¬æ¢æ•°æ®åˆ°tensor
        X_tensor = torch.LongTensor(X).to(device)
        y_tensor = torch.LongTensor(y).to(device)
        
        # å‰å‘ä¼ æ’­
        outputs = self.model(X_tensor)
        loss = self.criterion(outputs, y_tensor)
        
        # åå‘ä¼ æ’­
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        
        return loss.item()
    
    def generate_sequence(self, seed_sequence, length=100, temperature=1.0):
        """ç”ŸæˆéŸ³ç¬¦åºåˆ—ï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
        if self.model is None:
            raise ValueError("æ¨¡å‹æœªåŠ è½½æˆ–æœªè®­ç»ƒ")
        
        self.model.eval()
        
        # ç¡®ä¿ç§å­åºåˆ—é•¿åº¦æ­£ç¡®
        if len(seed_sequence) < self.seq_length:
            # ç”¨0å¡«å……åˆ°æ‰€éœ€é•¿åº¦
            seed_sequence = [0] * (self.seq_length - len(seed_sequence)) + list(seed_sequence)
        elif len(seed_sequence) > self.seq_length:
            # æˆªå–æœ€åseq_lengthä¸ªå…ƒç´ 
            seed_sequence = seed_sequence[-self.seq_length:]
        
        generated_sequence = list(seed_sequence)
        
        # ç”Ÿæˆæ–°çš„éŸ³ç¬¦
        with torch.no_grad():
            for _ in range(length):
                # é¢„æµ‹ä¸‹ä¸€ä¸ªå­—ç¬¦
                current_seq = generated_sequence[-self.seq_length:]
                x = torch.LongTensor([current_seq]).to(device)
                outputs = self.model(x)
                predictions = F.softmax(outputs / temperature, dim=1).cpu().numpy()[0]
                
                # é‡‡æ ·ä¸‹ä¸€ä¸ªå­—ç¬¦
                next_idx = np.random.choice(len(predictions), p=predictions)
                generated_sequence.append(next_idx)
        
        return generated_sequence
