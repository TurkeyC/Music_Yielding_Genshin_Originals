#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ€§èƒ½ä¼˜åŒ–å’Œç¼“å­˜ç®¡ç†å™¨
æä¾›æ¨¡å‹ç¼“å­˜ã€ç»“æœç¼“å­˜å’Œæ€§èƒ½ç›‘æ§åŠŸèƒ½
"""

import streamlit as st
import os
import json
import pickle
import hashlib
import time
import psutil
from datetime import datetime, timedelta
from pathlib import Path
import threading
import queue

class PerformanceOptimizer:
    def __init__(self):
        self.cache_dir = Path("hoyomusic_cache")
        self.cache_dir.mkdir(exist_ok=True)
        
        self.model_cache = {}
        self.result_cache = {}
        self.performance_metrics = {
            "generation_times": [],
            "memory_usage": [],
            "gpu_usage": [],
            "cache_hits": 0,
            "cache_misses": 0
        }
        
    def enable_caching(self):
        """å¯ç”¨ç¼“å­˜ä¼˜åŒ–"""
        st.cache_data.clear()
        st.cache_resource.clear()
        
    @st.cache_resource
    def load_model_cached(self, model_path):
        """ç¼“å­˜æ¨¡å‹åŠ è½½"""
        try:
            start_time = time.time()
            
            # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²åœ¨å†…å­˜ä¸­
            model_hash = self._get_file_hash(model_path)
            if model_hash in self.model_cache:
                self.performance_metrics["cache_hits"] += 1
                return self.model_cache[model_hash]
            
            # åŠ è½½æ¨¡å‹
            from model import HoyoMusicGenerator
            model = HoyoMusicGenerator()
            model.load_model(model_path)
            
            # ç¼“å­˜æ¨¡å‹
            self.model_cache[model_hash] = model
            self.performance_metrics["cache_misses"] += 1
            
            load_time = time.time() - start_time
            self.performance_metrics["generation_times"].append({
                "type": "model_load",
                "time": load_time,
                "timestamp": datetime.now().isoformat()
            })
            
            return model
            
        except Exception as e:
            st.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            return None
            
    @st.cache_data(ttl=3600)  # ç¼“å­˜1å°æ—¶
    def generate_music_cached(self, region, style, length, temperature, seed, model_path):
        """ç¼“å­˜éŸ³ä¹ç”Ÿæˆç»“æœ"""
        try:
            start_time = time.time()
            
            # ç”Ÿæˆç¼“å­˜é”®
            cache_key = self._generate_cache_key(region, style, length, temperature, seed)
            
            # æ£€æŸ¥ç¼“å­˜
            cached_result = self._get_cached_result(cache_key)
            if cached_result:
                self.performance_metrics["cache_hits"] += 1
                return cached_result
            
            # åŠ è½½æ¨¡å‹
            model = self.load_model_cached(model_path)
            if not model:
                return None
                
            # ç”ŸæˆéŸ³ä¹
            result = model.generate(
                length=length,
                temperature=temperature,
                seed=seed,
                region=region,
                style=style
            )
            
            # ç¼“å­˜ç»“æœ
            self._cache_result(cache_key, result)
            self.performance_metrics["cache_misses"] += 1
            
            generation_time = time.time() - start_time
            self.performance_metrics["generation_times"].append({
                "type": "music_generation",
                "time": generation_time,
                "timestamp": datetime.now().isoformat()
            })
            
            return result
            
        except Exception as e:
            st.error(f"âŒ éŸ³ä¹ç”Ÿæˆå¤±è´¥: {str(e)}")
            return None
            
    def monitor_system_resources(self):
        """ç›‘æ§ç³»ç»Ÿèµ„æºä½¿ç”¨"""
        try:
            # CPUå’Œå†…å­˜ä½¿ç”¨ç‡
            cpu_percent = psutil.cpu_percent(interval=1)
            memory = psutil.virtual_memory()
            
            metrics = {
                "timestamp": datetime.now().isoformat(),
                "cpu_percent": cpu_percent,
                "memory_percent": memory.percent,
                "memory_used_gb": memory.used / (1024**3),
                "memory_total_gb": memory.total / (1024**3)
            }
            
            # GPUä½¿ç”¨ç‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            try:
                import torch
                if torch.cuda.is_available():
                    gpu_memory = torch.cuda.memory_allocated() / (1024**3)
                    gpu_memory_total = torch.cuda.max_memory_allocated() / (1024**3)
                    metrics.update({
                        "gpu_memory_gb": gpu_memory,
                        "gpu_memory_total_gb": gpu_memory_total,
                        "gpu_utilization": gpu_memory / gpu_memory_total * 100 if gpu_memory_total > 0 else 0
                    })
            except:
                pass
                
            self.performance_metrics["memory_usage"].append(metrics)
            
            # ä¿æŒæœ€è¿‘100æ¡è®°å½•
            if len(self.performance_metrics["memory_usage"]) > 100:
                self.performance_metrics["memory_usage"] = self.performance_metrics["memory_usage"][-100:]
                
            return metrics
            
        except Exception as e:
            st.error(f"âŒ èµ„æºç›‘æ§å¤±è´¥: {str(e)}")
            return {}
            
    def create_performance_dashboard(self):
        """åˆ›å»ºæ€§èƒ½ç›‘æ§ä»ªè¡¨æ¿"""
        st.markdown("### âš¡ æ€§èƒ½ç›‘æ§")
        
        # å®æ—¶èµ„æºä½¿ç”¨
        current_metrics = self.monitor_system_resources()
        
        if current_metrics:
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                cpu_color = self._get_usage_color(current_metrics.get("cpu_percent", 0))
                st.markdown(f"""
                <div style="text-align: center; padding: 15px; background: rgba({cpu_color}, 0.2); border-radius: 10px;">
                    <h3 style="color: rgb({cpu_color});">ğŸ–¥ï¸ CPU</h3>
                    <h2 style="color: #fff;">{current_metrics.get("cpu_percent", 0):.1f}%</h2>
                </div>
                """, unsafe_allow_html=True)
                
            with col2:
                memory_color = self._get_usage_color(current_metrics.get("memory_percent", 0))
                st.markdown(f"""
                <div style="text-align: center; padding: 15px; background: rgba({memory_color}, 0.2); border-radius: 10px;">
                    <h3 style="color: rgb({memory_color});">ğŸ’¾ å†…å­˜</h3>
                    <h2 style="color: #fff;">{current_metrics.get("memory_percent", 0):.1f}%</h2>
                    <small>{current_metrics.get("memory_used_gb", 0):.1f}GB / {current_metrics.get("memory_total_gb", 0):.1f}GB</small>
                </div>
                """, unsafe_allow_html=True)
                
            with col3:
                gpu_utilization = current_metrics.get("gpu_utilization", 0)
                gpu_color = self._get_usage_color(gpu_utilization)
                st.markdown(f"""
                <div style="text-align: center; padding: 15px; background: rgba({gpu_color}, 0.2); border-radius: 10px;">
                    <h3 style="color: rgb({gpu_color});">ğŸ® GPU</h3>
                    <h2 style="color: #fff;">{gpu_utilization:.1f}%</h2>
                    <small>{current_metrics.get("gpu_memory_gb", 0):.1f}GB</small>
                </div>
                """, unsafe_allow_html=True)
                
            with col4:
                cache_ratio = self._calculate_cache_hit_ratio()
                cache_color = self._get_cache_color(cache_ratio)
                st.markdown(f"""
                <div style="text-align: center; padding: 15px; background: rgba({cache_color}, 0.2); border-radius: 10px;">
                    <h3 style="color: rgb({cache_color});">ğŸ“¦ ç¼“å­˜</h3>
                    <h2 style="color: #fff;">{cache_ratio:.1f}%</h2>
                    <small>å‘½ä¸­ç‡</small>
                </div>
                """, unsafe_allow_html=True)
                
        # æ€§èƒ½è¶‹åŠ¿å›¾
        self._create_performance_charts()
        
        # ä¼˜åŒ–å»ºè®®
        self._create_optimization_suggestions()
        
    def _create_performance_charts(self):
        """åˆ›å»ºæ€§èƒ½è¶‹åŠ¿å›¾"""
        if not self.performance_metrics["memory_usage"]:
            return
            
        import plotly.graph_objects as go
        from plotly.subplots import make_subplots
        
        # è·å–æœ€è¿‘æ•°æ®
        recent_data = self.performance_metrics["memory_usage"][-20:]
        timestamps = [datetime.fromisoformat(d["timestamp"]) for d in recent_data]
        
        # åˆ›å»ºå­å›¾
        fig = make_subplots(
            rows=2, cols=2,
            subplot_titles=('CPUä½¿ç”¨ç‡', 'å†…å­˜ä½¿ç”¨ç‡', 'GPUä½¿ç”¨ç‡', 'ç”Ÿæˆæ—¶é—´'),
            specs=[[{"secondary_y": False}, {"secondary_y": False}],
                   [{"secondary_y": False}, {"secondary_y": False}]]
        )
        
        # CPUä½¿ç”¨ç‡
        cpu_data = [d.get("cpu_percent", 0) for d in recent_data]
        fig.add_trace(
            go.Scatter(x=timestamps, y=cpu_data, name="CPU", line=dict(color="#FF6B6B")),
            row=1, col=1
        )
        
        # å†…å­˜ä½¿ç”¨ç‡
        memory_data = [d.get("memory_percent", 0) for d in recent_data]
        fig.add_trace(
            go.Scatter(x=timestamps, y=memory_data, name="å†…å­˜", line=dict(color="#4ECDC4")),
            row=1, col=2
        )
        
        # GPUä½¿ç”¨ç‡
        gpu_data = [d.get("gpu_utilization", 0) for d in recent_data]
        fig.add_trace(
            go.Scatter(x=timestamps, y=gpu_data, name="GPU", line=dict(color="#45B7D1")),
            row=2, col=1
        )
        
        # ç”Ÿæˆæ—¶é—´
        generation_times = [gt["time"] for gt in self.performance_metrics["generation_times"][-10:]]
        gen_timestamps = [datetime.fromisoformat(gt["timestamp"]) for gt in self.performance_metrics["generation_times"][-10:]]
        if generation_times:
            fig.add_trace(
                go.Scatter(x=gen_timestamps, y=generation_times, name="ç”Ÿæˆæ—¶é—´", line=dict(color="#96CEB4")),
                row=2, col=2
            )
        
        fig.update_layout(
            height=500,
            showlegend=False,
            template="plotly_dark",
            paper_bgcolor='rgba(0,0,0,0)',
            plot_bgcolor='rgba(0,0,0,0)',
            font=dict(color='white')
        )
        
        st.plotly_chart(fig, use_container_width=True)
        
    def _create_optimization_suggestions(self):
        """åˆ›å»ºä¼˜åŒ–å»ºè®®"""
        st.markdown("### ğŸ’¡ æ€§èƒ½ä¼˜åŒ–å»ºè®®")
        
        suggestions = []
        
        # åŸºäºç¼“å­˜å‘½ä¸­ç‡çš„å»ºè®®
        cache_ratio = self._calculate_cache_hit_ratio()
        if cache_ratio < 50:
            suggestions.append({
                "type": "warning",
                "title": "ç¼“å­˜å‘½ä¸­ç‡è¾ƒä½",
                "suggestion": "è€ƒè™‘å¢åŠ ç¼“å­˜å¤§å°æˆ–ä¼˜åŒ–ç¼“å­˜ç­–ç•¥"
            })
            
        # åŸºäºå†…å­˜ä½¿ç”¨çš„å»ºè®®
        if self.performance_metrics["memory_usage"]:
            avg_memory = sum(d.get("memory_percent", 0) for d in self.performance_metrics["memory_usage"][-10:]) / 10
            if avg_memory > 80:
                suggestions.append({
                    "type": "error",
                    "title": "å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜",
                    "suggestion": "å»ºè®®å…³é—­å…¶ä»–åº”ç”¨ç¨‹åºæˆ–å¢åŠ ç³»ç»Ÿå†…å­˜"
                })
            elif avg_memory > 60:
                suggestions.append({
                    "type": "warning",
                    "title": "å†…å­˜ä½¿ç”¨ç‡è¾ƒé«˜",
                    "suggestion": "æ³¨æ„ç›‘æ§å†…å­˜ä½¿ç”¨ï¼Œé¿å…ç³»ç»Ÿå¡é¡¿"
                })
                
        # åŸºäºç”Ÿæˆæ—¶é—´çš„å»ºè®®
        if self.performance_metrics["generation_times"]:
            avg_time = sum(gt["time"] for gt in self.performance_metrics["generation_times"][-5:]) / 5
            if avg_time > 30:
                suggestions.append({
                    "type": "warning",
                    "title": "ç”Ÿæˆæ—¶é—´è¾ƒé•¿",
                    "suggestion": "è€ƒè™‘ä½¿ç”¨GPUåŠ é€Ÿæˆ–å‡å°‘ç”Ÿæˆé•¿åº¦"
                })
                
        # æ˜¾ç¤ºå»ºè®®
        if suggestions:
            for suggestion in suggestions:
                if suggestion["type"] == "error":
                    st.error(f"ğŸš¨ {suggestion['title']}: {suggestion['suggestion']}")
                elif suggestion["type"] == "warning":
                    st.warning(f"âš ï¸ {suggestion['title']}: {suggestion['suggestion']}")
                else:
                    st.info(f"ğŸ’¡ {suggestion['title']}: {suggestion['suggestion']}")
        else:
            st.success("âœ… ç³»ç»Ÿæ€§èƒ½è‰¯å¥½ï¼Œæ— éœ€ä¼˜åŒ–å»ºè®®")
            
    def cleanup_cache(self, max_age_hours=24):
        """æ¸…ç†è¿‡æœŸç¼“å­˜"""
        try:
            cleaned = 0
            cutoff_time = datetime.now() - timedelta(hours=max_age_hours)
            
            # æ¸…ç†æ–‡ä»¶ç¼“å­˜
            for cache_file in self.cache_dir.glob("*.cache"):
                if datetime.fromtimestamp(cache_file.stat().st_mtime) < cutoff_time:
                    cache_file.unlink()
                    cleaned += 1
                    
            # æ¸…ç†å†…å­˜ç¼“å­˜
            self.result_cache.clear()
            
            st.success(f"ğŸ§¹ å·²æ¸…ç† {cleaned} ä¸ªè¿‡æœŸç¼“å­˜æ–‡ä»¶")
            
        except Exception as e:
            st.error(f"âŒ ç¼“å­˜æ¸…ç†å¤±è´¥: {str(e)}")
            
    def _get_file_hash(self, file_path):
        """è·å–æ–‡ä»¶å“ˆå¸Œå€¼"""
        hasher = hashlib.md5()
        with open(file_path, 'rb') as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hasher.update(chunk)
        return hasher.hexdigest()
        
    def _generate_cache_key(self, *args):
        """ç”Ÿæˆç¼“å­˜é”®"""
        key_string = "|".join(str(arg) for arg in args)
        return hashlib.md5(key_string.encode()).hexdigest()
        
    def _get_cached_result(self, cache_key):
        """è·å–ç¼“å­˜ç»“æœ"""
        # æ£€æŸ¥å†…å­˜ç¼“å­˜
        if cache_key in self.result_cache:
            return self.result_cache[cache_key]
            
        # æ£€æŸ¥æ–‡ä»¶ç¼“å­˜
        cache_file = self.cache_dir / f"{cache_key}.cache"
        if cache_file.exists():
            try:
                with open(cache_file, 'rb') as f:
                    result = pickle.load(f)
                # åŒæ—¶åŠ è½½åˆ°å†…å­˜ç¼“å­˜
                self.result_cache[cache_key] = result
                return result
            except:
                # ç¼“å­˜æ–‡ä»¶æŸåï¼Œåˆ é™¤
                cache_file.unlink()
                
        return None
        
    def _cache_result(self, cache_key, result):
        """ç¼“å­˜ç»“æœ"""
        # å†…å­˜ç¼“å­˜
        self.result_cache[cache_key] = result
        
        # æ–‡ä»¶ç¼“å­˜
        cache_file = self.cache_dir / f"{cache_key}.cache"
        try:
            with open(cache_file, 'wb') as f:
                pickle.dump(result, f)
        except:
            pass  # ç¼“å­˜å¤±è´¥ä¸å½±å“ä¸»è¦åŠŸèƒ½
            
    def _get_usage_color(self, usage_percent):
        """æ ¹æ®ä½¿ç”¨ç‡è·å–é¢œè‰²"""
        if usage_percent > 80:
            return "244, 67, 54"  # çº¢è‰²
        elif usage_percent > 60:
            return "255, 152, 0"  # æ©™è‰²
        else:
            return "76, 175, 80"  # ç»¿è‰²
            
    def _get_cache_color(self, hit_ratio):
        """æ ¹æ®ç¼“å­˜å‘½ä¸­ç‡è·å–é¢œè‰²"""
        if hit_ratio > 70:
            return "76, 175, 80"  # ç»¿è‰²
        elif hit_ratio > 40:
            return "255, 152, 0"  # æ©™è‰²
        else:
            return "244, 67, 54"  # çº¢è‰²
            
    def _calculate_cache_hit_ratio(self):
        """è®¡ç®—ç¼“å­˜å‘½ä¸­ç‡"""
        total = self.performance_metrics["cache_hits"] + self.performance_metrics["cache_misses"]
        if total == 0:
            return 0
        return (self.performance_metrics["cache_hits"] / total) * 100
        
    def save_performance_report(self):
        """ä¿å­˜æ€§èƒ½æŠ¥å‘Š"""
        try:
            report = {
                "generated_at": datetime.now().isoformat(),
                "metrics": self.performance_metrics,
                "system_info": {
                    "cpu_count": psutil.cpu_count(),
                    "memory_total_gb": psutil.virtual_memory().total / (1024**3),
                    "platform": os.name
                }
            }
            
            # æ·»åŠ GPUä¿¡æ¯
            try:
                import torch
                if torch.cuda.is_available():
                    report["system_info"]["gpu_name"] = torch.cuda.get_device_name(0)
                    report["system_info"]["gpu_memory_gb"] = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            except:
                pass
                
            report_file = Path("logs") / f"performance_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            report_file.parent.mkdir(exist_ok=True)
            
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
                
            st.success(f"ğŸ“Š æ€§èƒ½æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
            
        except Exception as e:
            st.error(f"âŒ ä¿å­˜æ€§èƒ½æŠ¥å‘Šå¤±è´¥: {str(e)}")

# å…¨å±€æ€§èƒ½ä¼˜åŒ–å™¨å®ä¾‹
performance_optimizer = PerformanceOptimizer()
